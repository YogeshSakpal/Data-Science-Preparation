{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions on KNN - \n",
    "\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of KNN - \n",
    "\n",
    "1. Simple & intuitive — The algorithm is very easy to understand and implement\n",
    "2. Memory based approach — Allows it to immediately adapt to new training data\n",
    "3. Variety of distance metrics — There is flexibility from the users side to use a distance metric which is best suited for their application (Euclidean, Minkowski, Manhattan, Hamming distance etc.)\n",
    "\n",
    "Hamming Distance: Calculate the distance between binary vectors.\n",
    "\n",
    "Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance.\n",
    "\n",
    "Minkowski Distance: Generalization of Euclidean and Manhattan distance.\n",
    "\n",
    "## Disadvantages of KNN - \n",
    "\n",
    "1. Computational complexity — As your training data increases, the speed at which calculations are made rapidly decrease\n",
    "2. Poor performance on imbalanced data — When majority of the data the model is being trained on represents 1 label then that label will have a high likelihood of being predicted\n",
    "3. Optimal value of K — If chosen incorrectly, the model will be under or overfitted to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN for classification: \n",
    "\n",
    "KNN can be used for classification in a supervised setting where we are given a dataset with target labels. For classification, KNN finds the k nearest data points in the training set and the target label is computed as the mode of the target label of these k nearest neighbours.\n",
    "\n",
    "## KNN for Regression:\n",
    "\n",
    "KNN can be used for regression in a supervised setting where we are given a dataset with continuous target values. For regression, KNN finds the k nearest data points in the training set and the target value is computed as the mean of the target value of these k nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitive to outliers: \n",
    "\n",
    "A single mislabeled example can change the class boundaries. This could specially be a bigger problem for larger dimensions, if there is an outlier in one dimension, since the average separation tends to be higher for higher dimensions (curse of dimensionality), outliers can have a bigger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value treatment: \n",
    "\n",
    "K-NN inherently has no capability of dealing with missing value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Value of K\n",
    "\n",
    "The choice of K is crucial for the model, if chosen incorrectly it can cause the model to be over / under fit. A K value too small will cause noise in the data to have a high influence on the prediction, however a K value too large will make it computationally expensive.\n",
    "The industry standard for choosing the optimal value of K is by taking the square root of N, where N is the total number of samples. Of course, take this with a grain of salt as it varies from problem to problem.\n",
    "You can experiment with various values of K and their associated accuracies. Common practices to determine the accuracy of a KNN model is to use confusion matrices, cross validation or F1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question on KNN\n",
    "\n",
    "### 1. Is KNN parametric or non-parametric?\n",
    "\n",
    "KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution.\n",
    "\n",
    "### 2. What is the K nearest neighbor classification based on?\n",
    "\n",
    "K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.\n",
    "\n",
    "### 3. What is meant by K Nearest Neighbor algorithm?\n",
    "\n",
    "A k-nearest-neighbor algorithm, often abbreviated k-nn, is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in\n",
    "\n",
    "### 4. How do you calculate KNN from K?\n",
    "\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.\n",
    "\n",
    "### 5. Why KNN is called lazy?\n",
    "\n",
    "\n",
    "Why is the k-nearest neighbors algorithm called “lazy”? Because it does no training at all when you supply the training data. At training time, all it is doing is storing the complete data set but it does not do any calculations at this point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
